{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27302faf",
   "metadata": {},
   "source": [
    "# Input/Output (I&O) Feature Script\n",
    "\n",
    "## Description\n",
    "This script processes massive MIMIC-IV event tables (`inputevents`, `outputevents`) to calculate total fluid volume flux per hospital admission.\n",
    "\n",
    "## Clinical Justification for HAPI Research\n",
    "Fluid balance is a critical determinant of skin integrity:\n",
    "* **Moisture :** High fluid output can lead to moisture-associated skin damage, weakening the skin barrier.\n",
    "* **Perfusion & Edema:** Positive fluid balance (fluid overload) causes tissue edema, increasing the diffusion distance for oxygen and nutrients to the skin.\n",
    "* **Dehydration:** Negative fluid balance reduces skin turgor and blood volume, impairing tissue perfusion.\n",
    "\n",
    "## Inputs & Outputs\n",
    "* **Inputs:** `inputevents.csv`, `outputevents.csv`, `hospitalwide_hapi_labels.csv` (for filtering)\n",
    "* **Output:** `io_feat.csv`\n",
    "* **Key Features:**\n",
    "    * `total_input_ml`: Total volume of fluids/meds administered.\n",
    "    * `total_output_ml`: Total volume of urine/drainage.\n",
    "    * `net_io_ml`: Net fluid balance (Input - Output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06f894bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b981ee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congiguration\n",
    "\n",
    "# Base directory for MIMIC-IV data files\n",
    "BASE_DIR = r\"D:\\School\\5141\"\n",
    "\n",
    "# Path to HAPI labels (useqd to filter to relevant encounters only for computational efficiency)\n",
    "LABEL_PATH = os.path.join(BASE_DIR, \"hospitalwide_hapi_labels.csv\")\n",
    "\n",
    "#MIMIC-IV IO event files\n",
    "INPUTEVENTS_PATH  = os.path.join(BASE_DIR, \"inputevents.csv\", \"inputevents.csv\")\n",
    "OUTPUTEVENTS_PATH = os.path.join(BASE_DIR, \"outputevents.csv\", \"outputevents.csv\")\n",
    "\n",
    "# Output file for  IO features\n",
    "OUTPUT_FEAT_PATH = os.path.join(BASE_DIR, \"io_feat.csv\")\n",
    "\n",
    "# Number of rows per chunk \n",
    "CHUNK_SIZE = 500_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be62163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hadm_list():\n",
    "    \"\"\"\n",
    "    Load the list of HADM_IDs from final HAPI label table. \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(LABEL_PATH, usecols=[\"hadm_id\"], low_memory=False)\n",
    "    return set(df[\"hadm_id\"].astype(\"Int64\").dropna().unique())\n",
    "\n",
    "def aggregate_stream(path, hadm_list, value_col, uom_col, out_col_name):\n",
    "    \"\"\"\n",
    "    Process MIMIC event table in chunks.\n",
    "    \n",
    "    Parameters:    \n",
    "    path : str\n",
    "        CSV path to either inputevents or outputevents.\n",
    "    hadm_list : set\n",
    "        Set of HADM_IDs to filter down to.\n",
    "    value_col : str\n",
    "        Column containing volume.\n",
    "    uom_col : str\n",
    "        Unit of measure column.\n",
    "    out_col_name : str\n",
    "        feature being created.\n",
    "\n",
    "    Returns    \n",
    "    pd.DataFrame\n",
    "        A table with:\n",
    "          hadm_id\n",
    "          summed volume in mL\n",
    "    \"\"\"\n",
    "    print(f\"Processing {out_col_name} from {path}...\")\n",
    "    chunks = []\n",
    "\n",
    "    # Load in chunks\n",
    "    reader = pd.read_csv(\n",
    "        path,\n",
    "        usecols=[\"hadm_id\", value_col, uom_col],\n",
    "        chunksize=CHUNK_SIZE,\n",
    "        low_memory=False\n",
    "    )\n",
    "\n",
    "    # Loop over each chunk\n",
    "    for i, chunk in enumerate(reader):\n",
    "\n",
    "        # Keep only rows with both hadm_id and the volume column\n",
    "        chunk = chunk.dropna(subset=[\"hadm_id\", value_col])\n",
    "\n",
    "        # Consistent type for merging\n",
    "        chunk[\"hadm_id\"] = chunk[\"hadm_id\"].astype(\"Int64\")\n",
    "\n",
    "        # Filter to admissions that appear in the HAPI label file\n",
    "        chunk = chunk[chunk[\"hadm_id\"].isin(hadm_list)]\n",
    "\n",
    "        # Keep only measurements in mL \n",
    "        if uom_col in chunk.columns:\n",
    "            chunk = chunk[chunk[uom_col].str.lower().str.contains(\"ml\", na=False)]\n",
    "\n",
    "        # If any rows remain, group and sum their volumes\n",
    "        if not chunk.empty:\n",
    "            grp = (\n",
    "                chunk.groupby(\"hadm_id\")[value_col]\n",
    "                .sum()\n",
    "                .rename(out_col_name)\n",
    "            )\n",
    "            chunks.append(grp)\n",
    "\n",
    "    # If nothing matched, return empty dataframe\n",
    "    if not chunks:\n",
    "        return pd.DataFrame(columns=[\"hadm_id\", out_col_name])\n",
    "\n",
    "    # Combine partial sums from all chunks and re-sum\n",
    "    final = (\n",
    "        pd.concat(chunks)\n",
    "        .groupby(\"hadm_id\")\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dc9158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Only look at admissions that appear in the HAPI label table\n",
    "    target_hadms = get_hadm_list()\n",
    "\n",
    "    # Build Input Features \n",
    "    inputs = aggregate_stream(\n",
    "        INPUTEVENTS_PATH,\n",
    "        target_hadms,\n",
    "        \"amount\",\n",
    "        \"amountuom\",\n",
    "        \"total_input_ml\"\n",
    "    )\n",
    "\n",
    "    # Build Output Features\n",
    "    outputs = aggregate_stream(\n",
    "        OUTPUTEVENTS_PATH,\n",
    "        target_hadms,\n",
    "        \"value\",\n",
    "        \"valueuom\",\n",
    "        \"total_output_ml\"\n",
    "    )\n",
    "\n",
    "    # Merge both based on HADM_ID\n",
    "    feat = inputs.merge(outputs, on=\"hadm_id\", how=\"outer\")\n",
    "\n",
    "    # Fill in missing data with 0 (missing data in IO context usually means ZERO volume)\n",
    "    feat[\"total_input_ml\"] = feat[\"total_input_ml\"].fillna(0)\n",
    "    feat[\"total_output_ml\"] = feat[\"total_output_ml\"].fillna(0)\n",
    "\n",
    "    # Compute net fluid balance \n",
    "    feat[\"net_io_ml\"] = feat[\"total_input_ml\"] - feat[\"total_output_ml\"]\n",
    "\n",
    "    # Save final feature file\n",
    "    feat.to_csv(OUTPUT_FEAT_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff8be2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing total_input_ml from D:\\School\\5141\\inputevents.csv\\inputevents.csv...\n",
      "Processing total_output_ml from D:\\School\\5141\\outputevents.csv\\outputevents.csv...\n"
     ]
    }
   ],
   "source": [
    "# Execute\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
