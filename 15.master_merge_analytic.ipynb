{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c3cf36d",
   "metadata": {},
   "source": [
    "## Master Analytic Dataset Builder\n",
    "\n",
    "**Description:**  \n",
    "    This script consolidates all engineered features from the entire pipeline   \n",
    "    into the single, comprehensive, row-per-admission analytic dataset   \n",
    "    (FINAL_HAPI_ANALYTIC.csv) required for HAPI model training and evaluation.  \n",
    "\n",
    "**Key Logic:**  \n",
    "    - Key ID: hadm_id (hospital admission ID)  \n",
    "    - Join Type: Left merge on hadm_id, anchoring the merge to the HAPI labels file.  \n",
    "    - Integrity: Includes error handling for missing feature files and checks for duplicate hadm_id values in the final output.  \n",
    "\n",
    "**Inputs:** \n",
    "    - hospitalwide_hapi_labels.csv (The primary cohort anchor)  \n",
    "    - All individual feature files (*_feat.csv) listed in the FEATURE_FILES configuration list.  \n",
    "\n",
    "**Output:**\n",
    "    - FINAL_HAPI_ANALYTIC.csv (The final merged dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c7279ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1567259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration\n",
    "BASE_DIR = r\"D:\\School\\5141\"        # <-- adjust if needed\n",
    "OUTPUT_NAME = \"FINAL_HAPI_ANALYTIC.csv\"\n",
    "\n",
    "# List of feature files to merge.\n",
    "# Comment out any that you don’t have yet.\n",
    "FEATURE_FILES = [\n",
    "    \"demographics_feat.csv\",\n",
    "    \"los_feat.csv\",\n",
    "    \"icu_transfer.csv\",\n",
    "    \"icu_procedures_feat.csv\",\n",
    "    \"io_feat.csv\",\n",
    "    \"procedures_feat.csv\",\n",
    "    \"vitals_feat.csv\",              \n",
    "    \"labs_feat.csv\",                \n",
    "    \"hcpcs_feat.csv\",               \n",
    "    \"emar_feat.csv\",                \n",
    "    \"poe_feat.csv\",                 \n",
    "    \"poe_detail_feat.csv\",          \n",
    "    \"prescriptions_feat.csv\",       \n",
    "    \"pharmacy_feat.csv\",            \n",
    "    \"feat_notes.csv\",               # NLP features / Braden-like risk\n",
    "    \"diagnoses_feat.csv\",           # ICD-based risk factors\n",
    "    \"medications_master_feat.csv\",  # merged EMAR/Presc/Pharm/POE\n",
    "    \"hospitalwide_hapi_labels.csv\"  # target labels (HAPI_STRUCTURED / UNSTRUCTURED / FINAL)\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bb575e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def full_path(fname: str):\n",
    "    \"\"\"Build full path for a file living in BASE_DIR.\"\"\"\n",
    "    return os.path.join(BASE_DIR, fname)\n",
    "\n",
    "\n",
    "def load_feature_table(fname: str):\n",
    "    \"\"\"\n",
    "    Load a single feature table, enforce hadm_id type, and\n",
    "    keep it ready for merging.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame if loaded successfully, else None.\n",
    "    \"\"\"\n",
    "    path = full_path(fname)\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "\n",
    "    if \"hadm_id\" not in df.columns:\n",
    "        raise ValueError(f\"{fname} does not contain 'hadm_id' column. Columns: {list(df.columns)}\")\n",
    "\n",
    "    # Ensure consistent nullable integer type for joins\n",
    "    df[\"hadm_id\"] = df[\"hadm_id\"].astype(\"Int64\")\n",
    "\n",
    "    # drop exact duplicate rows\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    after = len(df)\n",
    "    if after < before:\n",
    "        print(f\"   · Dropped {before - after} duplicate rows in {fname}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_on_hadm_id(dfs: list[pd.DataFrame]):\n",
    "    \"\"\"\n",
    "    Merge all DataFrames in `dfs` on hadm_id using left joins.\n",
    "    Assumes each df has a 'hadm_id' column.\n",
    "    \"\"\"\n",
    "    def _merge(left: pd.DataFrame, right: pd.DataFrame):\n",
    "        merged = pd.merge(left, right, on=\"hadm_id\", how=\"left\")\n",
    "        return merged\n",
    "\n",
    "    return reduce(_merge, dfs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c89cc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Saving merged analytic dataset to:\n",
      "   D:\\School\\5141\\FINAL_HAPI_ANALYTIC.csv\n"
     ]
    }
   ],
   "source": [
    "# Execute\n",
    "def main():\n",
    "\n",
    "    dfs: list[pd.DataFrame] = []\n",
    "\n",
    "    # Load each feature file\n",
    "    for fname in FEATURE_FILES:\n",
    "        df = load_feature_table(fname)\n",
    "        if df is not None:\n",
    "            dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        raise RuntimeError(\"No feature tables were loaded. \"\n",
    "                           \"Check BASE_DIR and FEATURE_FILES list.\")\n",
    "\n",
    "    # Sort so that labels (hospitalwide_hapi_labels.csv) are merged last (nice but not required)\n",
    "    dfs_sorted = sorted(\n",
    "        dfs,\n",
    "        key=lambda d: \"HAPI_FINAL\" in d.columns or \"has_HAPI\" in d.columns\n",
    "    )\n",
    "\n",
    "    # Merge everything on hadm_id\n",
    "    final = merge_on_hadm_id(dfs_sorted)\n",
    "\n",
    "    # Save final dataset\n",
    "    out_path = full_path(OUTPUT_NAME)\n",
    "    print(f\"\\n Saving merged analytic dataset to:\\n   {out_path}\")\n",
    "    final.to_csv(out_path, index=False)\n",
    "    print(f\" Done. Final shape: {final.shape}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
